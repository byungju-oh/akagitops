

import React, { useState, useEffect, useRef } from 'react';
import axios from 'axios';
import { toast } from 'react-toastify';
import '../styles/VoiceNavigation.css';

const AzureVoiceNavigation = ({ onRouteFound, onLocationUpdate, onServiceShutdown }) => {
  const [isActive, setIsActive] = useState(false);
  const [status, setStatus] = useState('ëŒ€ê¸° ì¤‘');
  const [isListening, setIsListening] = useState(false);
  const [pendingDestination, setPendingDestination] = useState(null); // í™•ì¸ ëŒ€ê¸° ì¤‘ì¸ ëª©ì ì§€
  
  const recognitionRef = useRef(null);
  const audioQueueRef = useRef([]);
  const isSpeakingRef = useRef(false);
  const isMountedRef = useRef(true);
  const currentLocationRef = useRef(null);

  useEffect(() => {
    isMountedRef.current = true;
    if (isActive) {
      log('ìŒì„± ì•ˆë‚´ ì„œë¹„ìŠ¤ ì‹œì‘');
      setupRecognition();
      startNavigationSequence();
    }
    return () => {
      if (isActive) {
        log('ìŒì„± ì•ˆë‚´ ì„œë¹„ìŠ¤ ì •ë¦¬');
        if (recognitionRef.current) recognitionRef.current.stop();
        isSpeakingRef.current = false;
        audioQueueRef.current = [];
        onServiceShutdown?.();
      }
      isMountedRef.current = false;
    };
  }, [isActive]);

  const log = (message) => console.log(`[INFO] ${new Date().toLocaleTimeString()}: ${message}`);

  const handleStartService = () => setIsActive(true);
  const handleStopService = () => {
    setIsActive(false);
    setStatus('ì•ˆë‚´ ì¢…ë£Œë¨');
    setPendingDestination(null);
    toast.info("ìŒì„± ì•ˆë‚´ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.");
  };
  
  const startNavigationSequence = async () => {
    setStatus('í˜„ì¬ ìœ„ì¹˜ í™•ì¸ ì¤‘...');
    const location = await getCurrentLocation();
    if (location && isMountedRef.current) {
      await speak('í˜„ì¬ ìœ„ì¹˜ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ëª©ì ì§€ë¥¼ ë§ì”€í•´ ì£¼ì„¸ìš”.');
      if (isMountedRef.current) startListening();
    } else if (isMountedRef.current) {
      await speak('ìœ„ì¹˜ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
      handleStopService();
    }
  };

  const getCurrentLocation = () => new Promise(resolve => {
    navigator.geolocation.getCurrentPosition(
      (pos) => {
        const loc = { lat: pos.coords.latitude, lng: pos.coords.longitude };
        currentLocationRef.current = loc;
        onLocationUpdate?.(loc);
        resolve(loc);
      },
      () => { toast.error('ìœ„ì¹˜ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.'); resolve(null); },
      { enableHighAccuracy: true, timeout: 10000 }
    );
  });

  const speak = (text) => new Promise(resolve => {
    audioQueueRef.current.push({ text, resolve });
    if (!isSpeakingRef.current) processAudioQueue();
  });

  const processAudioQueue = async () => {
    if (isSpeakingRef.current || audioQueueRef.current.length === 0) return;
    isSpeakingRef.current = true;
    const { text, resolve } = audioQueueRef.current.shift();
    if (!isMountedRef.current) return;
    setStatus(`ì•ˆë‚´: "${text}"`);
    try {
      const response = await axios.post('/api/tts', `text=${encodeURIComponent(text.trim())}`, {
        headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
        timeout: 15000,
      });
      const audioData = response.data.audio_data;
      const audioBlob = new Blob([Uint8Array.from(atob(audioData), c => c.charCodeAt(0))], { type: 'audio/wav' });
      const audio = new Audio(URL.createObjectURL(audioBlob));
      audio.onended = () => {
        isSpeakingRef.current = false;
        if (isMountedRef.current) processAudioQueue();
        resolve();
      };
      audio.play();
    } catch (error) {
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = 'ko-KR';
      utterance.onend = () => {
        isSpeakingRef.current = false;
        if (isMountedRef.current) processAudioQueue();
        resolve();
      };
      window.speechSynthesis.speak(utterance);
    }
  };

  const setupRecognition = () => {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      setStatus('ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
      return;
    }
    const recognition = new SpeechRecognition();
    recognition.lang = 'ko-KR';
    recognition.continuous = false;
    
    recognition.onstart = () => { if(isMountedRef.current) setIsListening(true); };
    recognition.onend = () => { if(isMountedRef.current) setIsListening(false); };
    recognition.onerror = () => { if(isMountedRef.current) setStatus('ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'); };
    
    recognition.onresult = (event) => {
      if(!isMountedRef.current) return;
      const transcript = event.results[0][0].transcript.trim();
      if (pendingDestination) {
        processConfirmation(transcript);
      } else {
        handleRecognitionResult(transcript);
      }
    };
    recognitionRef.current = recognition;
  };

  // --- ì‹ ê·œ ê¸°ëŠ¥: ì¸ì‹ ê²°ê³¼ ì²˜ë¦¬ ë° í™•ì¸ ìš”ì²­ ---
  const handleRecognitionResult = async (transcript) => {
    setPendingDestination(transcript);
    setStatus(`"${transcript}" ë§ë‚˜ìš”?`);
    await speak(`"${transcript}" ë§ë‚˜ìš”?`);
    if(isMountedRef.current) startListening();
  };

  // --- ì‹ ê·œ ê¸°ëŠ¥: 'ë„¤/ì•„ë‹ˆì˜¤' ë‹µë³€ ì²˜ë¦¬ ---
  const processConfirmation = async (answer) => {
    const affirmative = ['ë„¤', 'ì˜ˆ', 'ë§ì•„', 'ë§ì•„ìš”', 'ì‘', 'ê·¸ë˜'].some(word => answer.includes(word));
    const negative = ['ì•„ë‹ˆ', 'ì•„ë‹ˆìš”', 'í‹€ë ¤', 'í‹€ë ¸ì–´'].some(word => answer.includes(word));

    if (affirmative) {
        await speak(`ë„¤, "${pendingDestination}"(ìœ¼)ë¡œ ê²½ë¡œë¥¼ ì°¾ìŠµë‹ˆë‹¤.`);
        handleDestinationInput(pendingDestination);
        setPendingDestination(null);
    } else if (negative) {
        setPendingDestination(null);
        await speak('ì£„ì†¡í•©ë‹ˆë‹¤. ëª©ì ì§€ë¥¼ ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.');
        if(isMountedRef.current) startListening();
    } else {
        await speak('"ë„¤" ë˜ëŠ” "ì•„ë‹ˆì˜¤"ë¡œ ë‹µí•´ì£¼ì„¸ìš”.');
        if(isMountedRef.current) startListening();
    }
  };

  const startListening = () => {
    if (recognitionRef.current && !isListening) {
      try {
        recognitionRef.current.start();
      } catch(e) { log('ìŒì„± ì¸ì‹ ì‹œì‘ ì˜¤ë¥˜'); }
    }
  };

  const handleDestinationInput = async (destinationText) => {
    try {
      const places = await searchDestination(destinationText);
      if (!places || places.length === 0) {
        await speak('ëª©ì ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”.');
        startListening(); return;
      }
      
      const destCoords = { lat: parseFloat(places[0].y), lng: parseFloat(places[0].x) };
      const routeResponse = await axios.post('/safe-walking-route', {
        start_latitude: currentLocationRef.current.lat,
        start_longitude: currentLocationRef.current.lng,
        end_latitude: destCoords.lat,
        end_longitude: destCoords.lng,
      });

      if (routeResponse.data) {
        onRouteFound?.(routeResponse.data, currentLocationRef.current, destCoords, places[0].place_name);
        await speak(`${places[0].place_name}ê¹Œì§€ ì•ˆë‚´ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.`);
        handleStopService();
      } else { throw new Error('ê²½ë¡œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'); }
    } catch (error) {
      await speak('ê²½ë¡œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ëª©ì ì§€ë¥¼ ë§ì”€í•´ ì£¼ì„¸ìš”.');
      startListening();
    }
  };

  const searchDestination = async (query) => {
    try {
      const response = await axios.get('/search-location-combined', { params: { query } });
      return response.data.places || [];
    } catch (error) { return []; }
  };

  return (
    <div className="voice-navigation azure-voice">
      {!isActive ? (
        <div className="voice-controls">
          <button onClick={handleStartService} className="start-voice-btn azure-btn">
            ğŸ¤ ìŒì„± ì•ˆë‚´ ì‹œì‘
          </button>
        </div>
      ) : (
        <div className="navigation-status">
          <div className="nav-info">
            <h3>{status}</h3>
            {isListening && <div className="voice-level-meter"><div className="voice-level-bar active"></div><div className="voice-level-bar active"></div><div className="voice-level-bar active"></div></div>}
          </div>

          {/* --- ì‹ ê·œ ê¸°ëŠ¥: í™•ì¸ ë²„íŠ¼ UI --- */}
          {pendingDestination && !isListening && (
            <div className="confirmation-dialog">
              <p>"{pendingDestination}"ìœ¼ë¡œ ì•Œì•„ë“¤ì—ˆìŠµë‹ˆë‹¤. ë§ë‚˜ìš”?</p>
              <div className="confirmation-buttons">
                <button onClick={() => processConfirmation('ë„¤')} className="confirm-btn yes-btn">âœ”ï¸ ë„¤</button>
                <button onClick={() => processConfirmation('ì•„ë‹ˆì˜¤')} className="confirm-btn no-btn">âŒ ì•„ë‹ˆì˜¤</button>
              </div>
            </div>
          )}

          <button onClick={handleStopService} className="stop-nav-btn">
            ğŸš« ì•ˆë‚´ ì¢…ë£Œ
          </button>
        </div>
      )}
    </div>
  );
};

export default AzureVoiceNavigation;