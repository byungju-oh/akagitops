# chatbot_service.py
import os
import base64
import re
from typing import Optional, Tuple
from dotenv import load_dotenv
from openai import AzureOpenAI

load_dotenv()

class SmartRAGSystem:
    def __init__(self):
        # Azure OpenAI ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        self.endpoint = os.getenv("ENDPOINT_URL", "your-endpoint-url")
        self.deployment = os.getenv("DEPLOYMENT_NAME", "your_name")  # ëª¨ë¸ëª… ì—…ë°ì´íŠ¸
        self.search_endpoint = os.getenv("SEARCH_ENDPOINT", "your-search-endpoint")
        self.search_key = os.getenv("SEARCH_KEY", "your-search-key")
        self.search_index = os.getenv("SEARCH_INDEX_NAME", "your-index-name")
        self.api_key = os.getenv("AZURE_OPENAI_API_KEY", "your-key")
        
        # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = AzureOpenAI(
            azure_endpoint=self.endpoint,
            api_key=self.api_key,
            api_version="2024-08-01-preview",  # ìµœì‹  API ë²„ì „
        )
        
        # RAGê°€ ì ì ˆí•œ ë‹µë³€ì„ ëª»í•  ë•Œ ë‚˜ì˜¤ëŠ” íŒ¨í„´ë“¤ (í•œê¸€ + ì˜ì–´)
        self.inadequate_patterns = [
            # í•œê¸€ íŒ¨í„´
            "ìë£Œì— ì—†ëŠ”", "ìë£Œì—ì„œ ì°¾ì„ ìˆ˜ ì—†", "ì •ë³´ê°€ ì—†", "êµ¬ì²´ì ì¸ ì •ë³´ê°€ ë¶€ì¡±",
            "ìë£Œì— ëª…ì‹œë˜ì§€ ì•Š", "ì œê³µëœ ìë£Œë§Œìœ¼ë¡œëŠ”", "ìë£Œì—ëŠ” í¬í•¨ë˜ì§€ ì•Š",
            "ì¶”ê°€ ì •ë³´ê°€ í•„ìš”", "ìë£Œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†", "ìë£Œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†",
            "ì œê³µëœ ë¬¸ì„œì—ëŠ” ì—†", "ì°¸ê³ ìë£Œì— ì—†", "ìë£Œ ë²”ìœ„ë¥¼ ë²—ì–´",
            "ì£„ì†¡í•©ë‹ˆë‹¤", "ì£„ì†¡í•˜ì§€ë§Œ", "ë¯¸ì•ˆí•˜ì§€ë§Œ", "ì•ˆíƒ€ê¹ê²Œë„",
            "ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†", "ëª…í™•í•œ ë‹µë³€ì´ ì–´ë ¤", "êµ¬ì²´ì ì¸ ë‹µë³€ ë¶ˆê°€",
            "í•´ë‹¹ ì •ë³´ëŠ” ì—†", "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»", "ì ì ˆí•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†",
            
            # ì˜ì–´ íŒ¨í„´
            "not found in the", "information is not available", "no information", 
            "not mentioned in the", "not included in the", "not specified in the",
            "cannot find", "unable to find", "couldn't find", "can't find",
            "no relevant information", "insufficient information", "lack of information",
            "sorry", "i'm sorry", "i apologize", "unfortunately", "regrettably",
            "cannot provide", "unable to provide", "can't provide", "couldn't provide",
            "not possible to", "impossible to", "difficult to", "hard to",
            "no data", "no details", "no specific", "not clear", "unclear",
            "beyond the scope", "outside the scope", "not covered", "not addressed",
            "i don't know", "i'm not sure", "uncertain", "unsure"
        ]
        
        # í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
        self.validate_config()
    
    def validate_config(self):
        """í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ê²€ì¦"""
        required_vars = {
            "ENDPOINT_URL": self.endpoint,
            "DEPLOYMENT_NAME": self.deployment,
            "AZURE_OPENAI_API_KEY": self.api_key,
            "SEARCH_ENDPOINT": self.search_endpoint,
            "SEARCH_KEY": self.search_key,
            "SEARCH_INDEX_NAME": self.search_index
        }
        
        missing_vars = []
        for var_name, value in required_vars.items():
            if not value or value.startswith("your-"):
                missing_vars.append(var_name)
        
        if missing_vars:
            print(f"âš ï¸ ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”: {', '.join(missing_vars)}")
            print("ğŸ’¡ .env íŒŒì¼ì— ì‹¤ì œ ê°’ì„ ì„¤ì •í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¡œ ì§€ì •í•˜ì„¸ìš”.")
    
    def test_basic_connection(self):
        """ê¸°ë³¸ OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            print("ğŸ” ê¸°ë³¸ OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸...")
            response = self.client.chat.completions.create(
                model=self.deployment,
                messages=[
                    {"role": "system", "content": "ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}
                ],
                max_tokens=50,
                temperature=0.3
            )
            print("âœ… ê¸°ë³¸ OpenAI ì—°ê²° ì„±ê³µ!")
            return True
        except Exception as e:
            print(f"âŒ ê¸°ë³¸ OpenAI ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    def post_process_answer(self, answer: str) -> str:
        """RAG ë‹µë³€ì˜ ì°¸ì¡°ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ë³€ê²½"""
        if not answer:
            return answer
        
        # [doc1], [doc2] ë“±ì˜ ì°¸ì¡°ë¥¼ ì œê±°
        citation_patterns = [
            r'\[doc\d+\]',
            r'\[document\d+\]',
            r'\[source\d+\]',
            r'\[ë¬¸ì„œ\d+\]',
            r'\[ìë£Œ\d+\]',
            r'\[ì°¸ê³ \d+\]'
        ]
        
        processed_answer = answer
        for pattern in citation_patterns:
            processed_answer = re.sub(pattern, '', processed_answer)
        
        # ì—°ì†ëœ ê³µë°±ì´ë‚˜ ì¤„ë°”ê¿ˆ ì •ë¦¬
        processed_answer = re.sub(r'\s+', ' ', processed_answer)
        processed_answer = re.sub(r'\n\s*\n', '\n\n', processed_answer)
        
        return processed_answer.strip()
    
    def add_credibility_footer(self, answer: str, source: str) -> str:
        """ë‹µë³€ì— ì‹ ë¢°ì„± ì •ë³´ ì¶”ê°€"""
        
        footer_options = {
            "RAG": "ğŸ“š *ì„œìš¸ì‹œ ê³µì‹ ì‹±í¬í™€ ëŒ€ì‘ ë§¤ë‰´ì–¼ ê¸°ë°˜*",
            "ìˆ˜ë™ RAG": "ğŸ” ì„œìš¸ì‹œ ì•ˆì „ê´€ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼", 
            "í•˜ë“œì½”ë”©ëœ RAG": "ğŸ“‹ ì„œìš¸ì‹œ ê° êµ¬ì²­ ê³µì‹ ì—°ë½ì²˜ ë° ëŒ€ì‘ ì ˆì°¨",
            "ì¼ë°˜ LLM": "ğŸ§  *ì¼ë°˜ì ì¸ ì•ˆì „ ìƒì‹ ë° ê°€ì´ë“œë¼ì¸*"
        }
        
        footer = footer_options.get(source, "")
        
        if footer:
            return f"{answer}\n\n{footer}"
        
        return answer
    
    def try_rag_answer(self, query: str) -> Tuple[Optional[str], bool]:
        """Azure Search í†µí•© RAGë¡œ ë‹µë³€ ì‹œë„"""
        try:
            # í™˜ê²½ ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ìŠ¤í‚µ
            if self.search_endpoint.startswith("your-") or self.search_key.startswith("your-"):
                print("âš ï¸ Azure Search ì„¤ì •ì´ ëˆ„ë½ë˜ì–´ RAGë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return None, False
            
            print("ğŸ” RAG ë‹µë³€ ì‹œë„ ì¤‘...")
            
            chat_prompt = [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ì‹±í¬í™€ ì‹ ê³  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ì°¸ì¡° ë²ˆí˜¸ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": query
                }
            ]
            
            completion = self.client.chat.completions.create(
                model=self.deployment,
                messages=chat_prompt,
                max_tokens=800,
                temperature=0.3,
                extra_body={
                    "data_sources": [{
                        "type": "azure_search",
                        "parameters": {
                            "endpoint": self.search_endpoint,
                            "index_name": self.search_index,
                            "query_type": "simple",
                            "fields_mapping": {
                                "content_fields": ["chunk"],
                                "title_field": "title"
                            },
                            "in_scope": True,
                            "top_n_documents": 5,
                            "authentication": {
                                "type": "api_key",
                                "key": self.search_key
                            }
                        }
                    }]
                }
            )
            
            rag_answer = completion.choices[0].message.content
            print("âœ… RAG ë‹µë³€ ìƒì„± ì„±ê³µ!")
            return rag_answer, True
            
        except Exception as e:
            print(f"âŒ RAG ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return None, False
    
    def try_manual_rag(self, query: str) -> Tuple[Optional[str], bool]:
        """ìˆ˜ë™ RAG ë°©ì‹ìœ¼ë¡œ ì‹œë„"""
        try:
            # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
            if self.search_endpoint.startswith("your-") or self.search_key.startswith("your-"):
                print("âš ï¸ Azure Search ì„¤ì •ì´ ëˆ„ë½ë˜ì–´ ìˆ˜ë™ RAGë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return None, False
            
            print("ğŸ” ìˆ˜ë™ RAG ì‹œë„ ì¤‘...")
            
            from azure.search.documents import SearchClient
            from azure.core.credentials import AzureKeyCredential
            
            search_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=self.search_index,
                credential=AzureKeyCredential(self.search_key)
            )
            
            # ê²€ìƒ‰ ì‹¤í–‰
            search_results = search_client.search(
                search_text=query,
                top=5,
                select=["chunk_id", "title", "chunk"]
            )
            
            # ê²€ìƒ‰ ê²°ê³¼ ì¡°í•©
            context = "ì°¸ê³ ìë£Œ:\n\n"
            result_count = 0
            for result in search_results:
                result_count += 1
                context += f"ë¬¸ì„œ {result_count}:\n"
                context += f"ì œëª©: {result.get('title', 'N/A')}\n"
                context += f"ë‚´ìš©: {result.get('chunk', 'N/A')}\n\n"
            
            if result_count == 0:
                print("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return None, False
            
            print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ {result_count}ê°œ ë°œê²¬")
            
            # ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ë‹µë³€ ìƒì„±
            prompt = f"""
{context}

ì§ˆë¬¸: {query}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì°¸ì¡° ë²ˆí˜¸([doc1] ë“±)ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
            """
            
            response = self.client.chat.completions.create(
                model=self.deployment,
                messages=[
                    {
                        "role": "system",
                        "content": "ì‹±í¬í™€ ì‹ ê³  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìë£Œ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•˜ê³ , ì°¸ì¡° ë²ˆí˜¸ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=800,
                temperature=0.3
            )
            
            print("âœ… ìˆ˜ë™ RAG ë‹µë³€ ìƒì„± ì„±ê³µ!")
            return response.choices[0].message.content, True
            
        except Exception as e:
            print(f"âŒ ìˆ˜ë™ RAG ì‹¤íŒ¨: {e}")
            return None, False
    
    def try_hardcoded_rag(self, query: str) -> Tuple[Optional[str], bool]:
        """í•˜ë“œì½”ë”©ëœ RAG ë°ì´í„°ë¡œ ì‹œë„ (Azure Searchê°€ ì•ˆ ë  ë•Œ ë°±ì—…)"""
        try:
            print("ğŸ” í•˜ë“œì½”ë”©ëœ RAG ë°ì´í„° ì‚¬ìš©...")
            
            # í•˜ë“œì½”ë”©ëœ ì‹±í¬í™€ ê´€ë ¨ ì •ë³´
            hardcoded_data = {
                "ë™ì‘êµ¬": {
                    "ì—°ë½ì²˜": "02-820-1234",
                    "ë‹´ë‹¹ë¶€ì„œ": "ì•ˆì „ê±´ì„¤êµí†µêµ­ ë„ì‹œì•ˆì „ê³¼",
                    "íŠ¹ë³„ì‚¬í•­": "ë…¸ëŸ‰ì§„ì—­/ì‚¬ë‹¹ì—­ ì¸ê·¼ì€ ì„œìš¸êµí†µê³µì‚¬ ë™ì‹œ ì‹ ê³  í•„ìš”",
                    "ì›¹ì‚¬ì´íŠ¸": "https://www.dongjak.go.kr"
                },
                "ê°•ë‚¨êµ¬": {
                    "ì—°ë½ì²˜": "02-3423-1234",
                    "ë‹´ë‹¹ë¶€ì„œ": "ë„ì‹œì•ˆì „ê³¼",
                    "íŠ¹ë³„ì‚¬í•­": "ì§€í•˜ì² ì—­ ì¸ê·¼ íŠ¹ë³„ ê´€ë¦¬"
                },
                "ì¼ë°˜ì •ë³´": {
                    "ì‘ê¸‰ì—°ë½ì²˜": "119, 112",
                    "ì‹ ê³ ì‹œ_í•„ìš”ì •ë³´": ["ì •í™•í•œ ìœ„ì¹˜", "ì‹±í¬í™€ í¬ê¸°", "ê¹Šì´", "ì£¼ë³€ ìƒí™©", "ì—°ë½ì²˜"],
                    "ì•ˆì „ìˆ˜ì¹™": ["ì¦‰ì‹œ ì ‘ê·¼ ê¸ˆì§€", "ì£¼ë³€ ì‚¬ëŒë“¤ì—ê²Œ ì•Œë¦¼", "119 ì‹ ê³ "],
                    "ì¸¡ì •ë°©ë²•": "ì•ˆì „ê±°ë¦¬ì—ì„œ ëˆˆìœ¼ë¡œ ì¶”ì •, ì§ì ‘ ì¸¡ì • ê¸ˆì§€"
                }
            }
            
            # ê²€ìƒ‰ì–´ì— ë”°ë¥¸ ê´€ë ¨ ì •ë³´ ì°¾ê¸°
            relevant_info = []
            query_lower = query.lower()
            
            # ì§€ì—­ë³„ ì •ë³´
            for region, info in hardcoded_data.items():
                if region in query and region != "ì¼ë°˜ì •ë³´":
                    relevant_info.append(f"{region} ì‹±í¬í™€ ì‹ ê³  ì •ë³´: {info}")
            
            # í‚¤ì›Œë“œë³„ ì •ë³´
            if any(keyword in query for keyword in ["ì—°ë½ì²˜", "ì‹ ê³ ", "ì „í™”", "ë²ˆí˜¸"]):
                relevant_info.append(f"ê¸´ê¸‰ ì—°ë½ì²˜: {hardcoded_data['ì¼ë°˜ì •ë³´']['ì‘ê¸‰ì—°ë½ì²˜']}")
            
            if any(keyword in query for keyword in ["ì •ë³´", "í•„ìš”", "ì¤€ë¹„", "ì‹ ê³ ì„œ"]):
                relevant_info.append(f"ì‹ ê³ ì‹œ í•„ìš”ì •ë³´: {hardcoded_data['ì¼ë°˜ì •ë³´']['ì‹ ê³ ì‹œ_í•„ìš”ì •ë³´']}")
            
            if any(keyword in query for keyword in ["ì•ˆì „", "ì£¼ì˜", "ìˆ˜ì¹™"]):
                relevant_info.append(f"ì•ˆì „ìˆ˜ì¹™: {hardcoded_data['ì¼ë°˜ì •ë³´']['ì•ˆì „ìˆ˜ì¹™']}")
            
            if any(keyword in query for keyword in ["í¬ê¸°", "ì¸¡ì •", "ì–´ë–»ê²Œ"]):
                relevant_info.append(f"ì¸¡ì •ë°©ë²•: {hardcoded_data['ì¼ë°˜ì •ë³´']['ì¸¡ì •ë°©ë²•']}")
            
            if not relevant_info:
                # ê¸°ë³¸ ì •ë³´ ì œê³µ
                relevant_info = [
                    f"ì¼ë°˜ ì‘ê¸‰ì—°ë½ì²˜: {hardcoded_data['ì¼ë°˜ì •ë³´']['ì‘ê¸‰ì—°ë½ì²˜']}",
                    f"ê¸°ë³¸ ì•ˆì „ìˆ˜ì¹™: {hardcoded_data['ì¼ë°˜ì •ë³´']['ì•ˆì „ìˆ˜ì¹™']}"
                ]
            
            context = "ì°¸ê³ ìë£Œ:\n\n" + "\n".join(relevant_info)
            
            prompt = f"""
{context}

ì§ˆë¬¸: {query}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì‹±í¬í™€ ì‹ ê³  ì „ë¬¸ê°€ë¡œì„œ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
            """
            
            response = self.client.chat.completions.create(
                model=self.deployment,
                messages=[
                    {
                        "role": "system",
                        "content": "ì‹±í¬í™€ ì‹ ê³  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=800,
                temperature=0.3
            )
            
            print("âœ… í•˜ë“œì½”ë”©ëœ RAG ë‹µë³€ ìƒì„± ì„±ê³µ!")
            return response.choices[0].message.content, True
            
        except Exception as e:
            print(f"âŒ í•˜ë“œì½”ë”©ëœ RAG ì‹¤íŒ¨: {e}")
            return None, False
    
    def is_inadequate_answer(self, answer: str) -> bool:
        """RAG ë‹µë³€ì´ ë¶€ì ì ˆí•œì§€ íŒë‹¨ (í•œê¸€ + ì˜ì–´)"""
        if not answer:
            return True
        
        answer_lower = answer.lower()
        
        # ë¶€ì ì ˆí•œ íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        for pattern in self.inadequate_patterns:
            if pattern in answer_lower:
                print(f"ğŸ” ë¶€ì ì ˆ íŒ¨í„´ ê°ì§€: '{pattern}'")
                return True
        
        # ë‹µë³€ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° (50ì ë¯¸ë§Œ)
        if len(answer.strip()) < 50:
            print(f"ğŸ” ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ: {len(answer.strip())}ì")
            return True
        
        return False
    
    def answer_with_general_llm(self, query: str) -> str:
        """ì¼ë°˜ LLMìœ¼ë¡œ ë‹µë³€"""
        try:
            print("ğŸ” ì¼ë°˜ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±...")
            
            response = self.client.chat.completions.create(
                model=self.deployment,
                messages=[
                    {
                        "role": "system",
                        "content": """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì‹±í¬í™€ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
íŠ¹íˆ ì•ˆì „ ìˆ˜ì¹™, ì‹ ê³  ë°©ë²•, ì˜ˆë°© ì¡°ì¹˜ ë“±ì— ëŒ€í•´ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”."""
                    },
                    {
                        "role": "user",
                        "content": query
                    }
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            print("âœ… ì¼ë°˜ LLM ë‹µë³€ ìƒì„± ì„±ê³µ!")
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âŒ ì¼ë°˜ LLM ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return """ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. 

ë‹¤ìŒê³¼ ê°™ì´ ì‹œë„í•´ë³´ì„¸ìš”:
â€¢ ì ì‹œ í›„ ë‹¤ì‹œ ì§ˆë¬¸í•´ë³´ì„¸ìš”
â€¢ ì§ˆë¬¸ì„ ë” ê°„ë‹¨í•˜ê²Œ ë°”ê¿”ë³´ì„¸ìš”
â€¢ ê¸´ê¸‰í•œ ê²½ìš° 119 ë˜ëŠ” 120ìœ¼ë¡œ ì—°ë½í•˜ì„¸ìš”

ğŸ” **ìš°ë¦¬ ì„œë¹„ìŠ¤ ê¸°ëŠ¥**
â€¢ ì‹¤ì‹œê°„ ìœ„í—˜ë„ ì˜ˆì¸¡
â€¢ ìœ„í—˜ì§€ì—­ ì§€ë„ í™•ì¸
â€¢ ì•ˆì „ ê²½ë¡œ ì•ˆë‚´

ì„œë¹„ìŠ¤ ì´ìš©ì— ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤."""
    
    def smart_answer(self, query: str, image_data: Optional[str] = None) -> Tuple[str, str]:
        """ìŠ¤ë§ˆíŠ¸ ë‹µë³€ ì‹œìŠ¤í…œ - ê°œì„ ëœ ì°¸ì¡° ì²˜ë¦¬"""
        
        print(f"ğŸ¤” ì§ˆë¬¸: {query}")
        print("-" * 60)
        
        # ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
        if not self.test_basic_connection():
            return "ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.", "ì˜¤ë¥˜"
        
        # 1ë‹¨ê³„: Azure Search í†µí•© RAG ì‹œë„
        print("ğŸ” 1ë‹¨ê³„: Azure Search í†µí•© RAG ì‹œë„")
        rag_answer, success = self.try_rag_answer(query)
        
        if success and rag_answer:
            print("âœ… RAG ë‹µë³€ ìƒì„± ì„±ê³µ")
            if not self.is_inadequate_answer(rag_answer):
                print("âœ… RAG ë‹µë³€ì´ ì ì ˆí•¨ â†’ RAG ë‹µë³€ ì‚¬ìš©")
                # ì°¸ì¡° í›„ì²˜ë¦¬ ì ìš©
                processed_answer = self.post_process_answer(rag_answer)
                final_answer = self.add_credibility_footer(processed_answer, "RAG")
                return final_answer, "RAG"
            else:
                print("âš ï¸ RAG ë‹µë³€ì´ ë¶€ì ì ˆí•¨")
        
        # 2ë‹¨ê³„: ìˆ˜ë™ RAG ì‹œë„
        print("ğŸ” 2ë‹¨ê³„: ìˆ˜ë™ RAG ì‹œë„")
        manual_answer, manual_success = self.try_manual_rag(query)
        
        if manual_success and manual_answer:
            print("âœ… ìˆ˜ë™ RAG ë‹µë³€ ìƒì„± ì„±ê³µ")
            if not self.is_inadequate_answer(manual_answer):
                print("âœ… ìˆ˜ë™ RAG ë‹µë³€ì´ ì ì ˆí•¨ â†’ ìˆ˜ë™ RAG ë‹µë³€ ì‚¬ìš©")
                processed_answer = self.post_process_answer(manual_answer)
                final_answer = self.add_credibility_footer(processed_answer, "ìˆ˜ë™ RAG")
                return final_answer, "ìˆ˜ë™ RAG"
            else:
                print("âš ï¸ ìˆ˜ë™ RAG ë‹µë³€ë„ ë¶€ì ì ˆí•¨")
        
        # 3ë‹¨ê³„: í•˜ë“œì½”ë”©ëœ RAG ì‹œë„
        print("ğŸ” 3ë‹¨ê³„: í•˜ë“œì½”ë”©ëœ RAG ì‹œë„")
        hardcoded_answer, hardcoded_success = self.try_hardcoded_rag(query)
        
        if hardcoded_success and hardcoded_answer:
            print("âœ… í•˜ë“œì½”ë”©ëœ RAG ë‹µë³€ ìƒì„± ì„±ê³µ")
            if not self.is_inadequate_answer(hardcoded_answer):
                print("âœ… í•˜ë“œì½”ë”©ëœ RAG ë‹µë³€ì´ ì ì ˆí•¨ â†’ í•˜ë“œì½”ë”©ëœ RAG ë‹µë³€ ì‚¬ìš©")
                final_answer = self.add_credibility_footer(hardcoded_answer, "í•˜ë“œì½”ë”©ëœ RAG")
                return final_answer, "í•˜ë“œì½”ë”©ëœ RAG"
        
        # 4ë‹¨ê³„: ì¼ë°˜ LLMìœ¼ë¡œ ì „í™˜
        print("ğŸ”„ 4ë‹¨ê³„: ì¼ë°˜ LLMìœ¼ë¡œ ì „í™˜")
        print("ğŸ’¡ RAGì— ì ì ˆí•œ ì •ë³´ê°€ ì—†ì–´ ì¼ë°˜ AI ì§€ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
        
        general_answer = self.answer_with_general_llm(query)
        final_answer = self.add_credibility_footer(general_answer, "ì¼ë°˜ LLM")
        return final_answer, "ì¼ë°˜ LLM"

# ì „ì—­ RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
rag_system = SmartRAGSystem()