# backend/chatbot_service.py - ì‹±í¬í™€ ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€
import os
import re
from typing import Tuple, Optional
from openai import AzureOpenAI
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from dotenv import load_dotenv
from sinkhole_analysis_service import sinkhole_analyzer

load_dotenv()

class EnhancedRAGSystem:
    """ì‹±í¬í™€ ë¶„ì„ ê¸°ëŠ¥ì´ í†µí•©ëœ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """Azure OpenAI ë° Search í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            # Azure OpenAI ì„¤ì •
            self.client = AzureOpenAI(
                api_key=os.getenv("AZURE_OPENAI_KEY"),
                api_version="2024-02-01",
                azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
            )
            self.deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4")
            
            # Azure Search ì„¤ì •
            self.search_endpoint = os.getenv("AZURE_SEARCH_ENDPOINT", "your-search-service")
            self.search_key = os.getenv("AZURE_SEARCH_KEY", "your-search-key")
            self.index_name = os.getenv("AZURE_SEARCH_INDEX", "sinkhole-docs")
            
            # ë¶€ì ì ˆí•œ ë‹µë³€ íŒ¨í„´
            self.inadequate_patterns = [
                "i don't have", "i cannot", "i'm unable", 
                "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤", "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤",
                "ë„ì›€ë“œë¦´ ìˆ˜", "sorry", "unfortunately"
            ]
            
            print("âœ… Enhanced RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ Enhanced RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.client = None
    
    def smart_answer(self, query: str, image_data: Optional[str] = None) -> Tuple[str, str]:
        """
        ìŠ¤ë§ˆíŠ¸ ë‹µë³€ ì‹œìŠ¤í…œ - ì´ë¯¸ì§€ ë¶„ì„ ê¸°ëŠ¥ í†µí•©
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            image_data: Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„° (ì„ íƒì‚¬í•­)
            
        Returns:
            Tuple[str, str]: (ë‹µë³€, ì†ŒìŠ¤)
        """
        
        print(f"ğŸ¤” ì§ˆë¬¸: {query}")
        if image_data:
            print("ğŸ“¸ ì´ë¯¸ì§€ ì²¨ë¶€ë¨ - ì‹±í¬í™€ ë¶„ì„ ì§„í–‰")
        print("-" * 60)
        
        # 1. ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ë¨¼ì € ì‹±í¬í™€ ë¶„ì„ ìˆ˜í–‰
        if image_data and sinkhole_analyzer.is_available:
            return self._handle_image_analysis(query, image_data)
        
        # 2. í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì²˜ë¦¬ (ê¸°ì¡´ RAG ë¡œì§)
        return self._handle_text_query(query)
    
    def _handle_image_analysis(self, query: str, image_data: str) -> Tuple[str, str]:
        """ì´ë¯¸ì§€ ë¶„ì„ì„ í†µí•œ ì‹±í¬í™€ íƒì§€ ì²˜ë¦¬"""
        
        try:
            # Azure Custom Visionìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ì„
            is_sinkhole, confidence, analysis_result = sinkhole_analyzer.analyze_image(image_data)
            
            # ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì‘ë‹µ ìƒì„±
            if is_sinkhole and confidence >= 0.7:  # 70% ì´ìƒ í™•ë¥ 
                print(f"ğŸš¨ ì‹±í¬í™€ íƒì§€! í™•ë¥ : {confidence:.2%}")
                return self._generate_sinkhole_report_response(confidence, analysis_result)
            
            elif confidence > 0.0:  # 50-70% í™•ë¥  (ë‚®ì€ í™•ë¥ )
                print(f"âš ï¸ ë¶ˆí™•ì‹¤í•œ ê²°ê³¼: {confidence:.2%}")
                return self._generate_uncertain_response(confidence, query)
            
            else:  # ì‹±í¬í™€ì´ ì•„ë‹Œ ê²½ìš°
                print("âœ… ì‹±í¬í™€ì´ ì•„ë‹Œ ê²ƒìœ¼ë¡œ íŒë‹¨")
                return self._generate_non_sinkhole_response(query)
                
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return self._generate_analysis_error_response(query)
    
    def _generate_sinkhole_report_response(self, confidence: float, analysis_result: dict) -> Tuple[str, str]:
        """70% ì´ìƒ í™•ë¥ ë¡œ ì‹±í¬í™€ì´ íƒì§€ëœ ê²½ìš°ì˜ ì‘ë‹µ"""
        
        # RAGì—ì„œ ì‹±í¬í™€ ì‹ ê³  ì ˆì°¨ ì •ë³´ ê²€ìƒ‰
        report_info = self._get_sinkhole_report_procedure()
        
        response = f"""ğŸš¨ **ì‹±í¬í™€ì´ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤!** (í™•ë¥ : {confidence:.1%})

âš ï¸ **ì¦‰ì‹œ ì•ˆì „ ì¡°ì¹˜ë¥¼ ì·¨í•´ì£¼ì„¸ìš”:**
1. í•´ë‹¹ ì§€ì—­ì—ì„œ ì¦‰ì‹œ ëŒ€í”¼í•˜ì„¸ìš”
2. ì£¼ë³€ ì‚¬ëŒë“¤ì—ê²Œ ìœ„í—˜ì„ ì•Œë ¤ì£¼ì„¸ìš”  
3. ì ‘ê·¼ì„ ì°¨ë‹¨í•˜ê³  ì•ˆì „ê±°ë¦¬ë¥¼ ìœ ì§€í•˜ì„¸ìš”

ğŸ“ **ê¸´ê¸‰ ì‹ ê³  ì—°ë½ì²˜:**
â€¢ 119 (ì†Œë°©ì„œ - ì‘ê¸‰ìƒí™©)
â€¢ 112 (ê²½ì°°ì„œ - êµí†µí†µì œ)
â€¢ 120 (ë‹¤ì‚°ì½œì„¼í„° - ì¼ë°˜ì‹ ê³ )

{report_info}

ğŸ” **AI ë¶„ì„ ê²°ê³¼:**
â€¢ íƒì§€ëœ ê°ì²´ ìˆ˜: {analysis_result.get('total_detections', 0)}ê°œ
â€¢ ë¶„ì„ ì‹ ë¢°ë„: {confidence:.1%}
â€¢ ì´ë¯¸ì§€ í¬ê¸°: {analysis_result.get('image_dimensions', {}).get('width', 0)}x{analysis_result.get('image_dimensions', {}).get('height', 0)}

âš ï¸ ë³¸ ë¶„ì„ ê²°ê³¼ëŠ” AI ê¸°ë°˜ ì°¸ê³ ìë£Œì´ë©°, í˜„ì¥ ì „ë¬¸ê°€ì˜ ì •í™•í•œ íŒë‹¨ì´ í•„ìš”í•©ë‹ˆë‹¤."""

        return response, "ì‹±í¬í™€ AI ë¶„ì„"
    
    def _generate_uncertain_response(self, confidence: float, query: str) -> Tuple[str, str]:
        """50-70% í™•ë¥ ì˜ ë¶ˆí™•ì‹¤í•œ ê²½ìš° ì‘ë‹µ"""
        
        response = f"""ğŸ¤” **ë¶„ì„ ê²°ê³¼ê°€ ë¶ˆí™•ì‹¤í•©ë‹ˆë‹¤** (í™•ë¥ : {confidence:.1%})

í˜„ì¬ ì—…ë¡œë“œí•˜ì‹  ì´ë¯¸ì§€ì—ì„œ ì‹±í¬í™€ íŠ¹ì§•ì´ ì¼ë¶€ ê°ì§€ë˜ì—ˆì§€ë§Œ, í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ğŸ’¡ **ê¶Œì¥ì‚¬í•­:**
1. ë” ì„ ëª…í•œ ì‚¬ì§„ìœ¼ë¡œ ë‹¤ì‹œ ì´¬ì˜í•´ë³´ì„¸ìš”
2. ë‹¤ì–‘í•œ ê°ë„ì—ì„œ ì¶”ê°€ ì‚¬ì§„ì„ ì´¬ì˜í•˜ì„¸ìš”
3. ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì•ˆì „ì„ ìœ„í•´ ì‹ ê³ ë¥¼ ê³ ë ¤í•˜ì„¸ìš”

ğŸ“ **ë¬¸ì˜ ì—°ë½ì²˜:**
â€¢ ê´€í•  êµ¬ì²­ ì•ˆì „ê´€ë¦¬ê³¼
â€¢ 120 (ë‹¤ì‚°ì½œì„¼í„°)

ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"""

        return response, "ë¶€ë¶„ íƒì§€"
    
    def _generate_non_sinkhole_response(self, query: str) -> Tuple[str, str]:
        """ì‹±í¬í™€ì´ ì•„ë‹Œ ê²ƒìœ¼ë¡œ íŒë‹¨ëœ ê²½ìš° ì‘ë‹µ"""
        
        # ì‚¬ìš©ì ì§ˆë¬¸ì— ë”°ë¥¸ ë§ì¶¤ ì‘ë‹µ
        if any(word in query.lower() for word in ["ì‹ ê³ ", "ì–´ë””", "ì—°ë½ì²˜"]):
            additional_info = "\n\ní˜¹ì‹œ ë‹¤ë¥¸ ì•ˆì „ ë¬¸ì œê°€ ìˆìœ¼ì‹œë‹¤ë©´ ê´€í•  êµ¬ì²­ì´ë‚˜ 120 ë‹¤ì‚°ì½œì„¼í„°ë¡œ ë¬¸ì˜í•˜ì„¸ìš”."
        else:
            additional_info = "\n\nì‹±í¬í™€ ê´€ë ¨ ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!"
        
        response = f"""âœ… **ë¶„ì„ ê²°ê³¼: ì‹±í¬í™€ì´ ì•„ë‹Œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤**

ì—…ë¡œë“œí•´ì£¼ì‹  ì´ë¯¸ì§€ë¥¼ AIë¡œ ë¶„ì„í•œ ê²°ê³¼, ì‹±í¬í™€ì˜ íŠ¹ì§•ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

ğŸ” **í•˜ì§€ë§Œ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš° ì¶”ê°€ í™•ì¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤:**
â€¢ ë„ë¡œë‚˜ ë³´ë„ì— ê· ì—´ì´ë‚˜ ì¹¨í•˜ê°€ ë³´ì´ëŠ” ê²½ìš°
â€¢ ì§€ë©´ì—ì„œ ë¬¼ì´ ìƒˆì–´ ë‚˜ì˜¤ëŠ” ê²½ìš°  
â€¢ ì£¼ë³€ì—ì„œ ì´ìƒí•œ ì†Œë¦¬ê°€ ë‚˜ëŠ” ê²½ìš°

{additional_info}"""

        return response, "ì •ìƒ ë¶„ì„"
    
    def _generate_analysis_error_response(self, query: str) -> Tuple[str, str]:
        """ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜ ë°œìƒì‹œ ì‘ë‹µ"""
        
        response = """âŒ **ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤**

ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì´ë¯¸ì§€ ë¶„ì„ ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.

ğŸ’¡ **ëŒ€ì•ˆ ë°©ë²•:**
1. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”
2. ì´ë¯¸ì§€ íŒŒì¼ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ë‹¤ì‹œ ì—…ë¡œë“œí•´ë³´ì„¸ìš”
3. í…ìŠ¤íŠ¸ë¡œ ìƒí™©ì„ ì„¤ëª…í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤

ğŸ“ **ê¸´ê¸‰ìƒí™©ì‹œ:**
â€¢ 119 (ì‘ê¸‰ìƒí™©)
â€¢ 120 (ë‹¤ì‚°ì½œì„¼í„°)
â€¢ ê´€í•  êµ¬ì²­ ì•ˆì „ê´€ë¦¬ê³¼

í…ìŠ¤íŠ¸ë¡œë„ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"""

        return response, "ë¶„ì„ ì˜¤ë¥˜"
    
    def _get_sinkhole_report_procedure(self) -> str:
        """RAGì—ì„œ ì‹±í¬í™€ ì‹ ê³  ì ˆì°¨ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        
        try:
            # Azure Searchë‚˜ í•˜ë“œì½”ë”©ëœ ì •ë³´ì—ì„œ ì‹ ê³  ì ˆì°¨ ê²€ìƒ‰
            report_procedure = """
ğŸ“‹ **ì‹ ê³ ì‹œ í•„ìš”í•œ ì •ë³´:**
â€¢ ì •í™•í•œ ìœ„ì¹˜ (ì£¼ì†Œ, ê·¼ì²˜ ê±´ë¬¼ëª…, GPS ì¢Œí‘œ)
â€¢ ì‹±í¬í™€ í¬ê¸° (ì§€ë¦„, ê¹Šì´ - ì•ˆì „ê±°ë¦¬ì—ì„œ ì¶”ì •)
â€¢ ë°œê²¬ ì‹œê°„
â€¢ ì£¼ë³€ ìƒí™© (ë„ë¡œ, ê±´ë¬¼, ì°¨ëŸ‰ ë“±)
â€¢ ì‹ ê³ ì ì—°ë½ì²˜

ğŸ“ **ì‹ ê³  ì ˆì°¨:**
1. 119 ì‹ ê³  í›„ í˜„ì¥ ì•ˆì „ í™•ë³´
2. ê´€í•  êµ¬ì²­ ì•ˆì „ê´€ë¦¬ê³¼ì— ìƒì„¸ ì‹ ê³ 
3. í•„ìš”ì‹œ ì‚¬ì§„ ë° ë™ì˜ìƒ ì´¬ì˜ (ì•ˆì „ê±°ë¦¬ì—ì„œ)
4. ë³µêµ¬ ì™„ë£Œê¹Œì§€ ì ‘ê·¼ ê¸ˆì§€ í‘œì‹œ ìœ ì§€

ğŸ¢ **ì„œìš¸ì‹œ ê° êµ¬ì²­ ì—°ë½ì²˜:**
â€¢ ë™ì‘êµ¬: 02-820-1234 (ì•ˆì „ê±´ì„¤êµí†µêµ­)
â€¢ ê°•ë‚¨êµ¬: 02-3423-1234 (ë„ì‹œì•ˆì „ê³¼)  
â€¢ ê¸°íƒ€ êµ¬ì²­: 120ìœ¼ë¡œ ë¬¸ì˜í•˜ì—¬ ì—°ê²°"""
            
            return report_procedure
            
        except Exception as e:
            print(f"âŒ ì‹ ê³  ì ˆì°¨ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return """
ğŸ“ **ê¸°ë³¸ ì‹ ê³  ì—°ë½ì²˜:**
â€¢ 119 (ì‘ê¸‰ìƒí™©)
â€¢ 120 (ë‹¤ì‚°ì½œì„¼í„°)
â€¢ ê´€í•  êµ¬ì²­ ì•ˆì „ê´€ë¦¬ê³¼"""
    
    def _handle_text_query(self, query: str) -> Tuple[str, str]:
        """í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì²˜ë¦¬ (ê¸°ì¡´ RAG ë¡œì§)"""
        
        # ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
        if not self.test_basic_connection():
            return "ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.", "ì—°ê²° ì˜¤ë¥˜"
        
        # 1. RAG ì‹œë„
        answer, rag_success = self.try_rag_answer(query)
        if rag_success and answer and not self.is_inadequate_answer(answer):
            processed_answer = self.post_process_answer(answer)
            final_answer = self.add_credibility_footer(processed_answer, "RAG")
            return final_answer, "RAG"
        
        # 2. ìˆ˜ë™ RAG ì‹œë„  
        manual_answer, manual_success = self.try_manual_rag(query)
        if manual_success and manual_answer and not self.is_inadequate_answer(manual_answer):
            processed_answer = self.post_process_answer(manual_answer)
            final_answer = self.add_credibility_footer(processed_answer, "ìˆ˜ë™ RAG")
            return final_answer, "ìˆ˜ë™ RAG"
        
        # 3. í•˜ë“œì½”ë”©ëœ RAG ì‹œë„
        hardcoded_answer, hardcoded_success = self.try_hardcoded_rag(query)
        if hardcoded_success and hardcoded_answer and not self.is_inadequate_answer(hardcoded_answer):
            processed_answer = self.post_process_answer(hardcoded_answer)
            final_answer = self.add_credibility_footer(processed_answer, "í•˜ë“œì½”ë”©ëœ RAG")
            return final_answer, "í•˜ë“œì½”ë”©ëœ RAG"
        
        # 4. ì¼ë°˜ LLM ì‚¬ìš©
        llm_answer = self.answer_with_general_llm(query)
        processed_answer = self.post_process_answer(llm_answer)
        final_answer = self.add_credibility_footer(processed_answer, "ì¼ë°˜ LLM")
        return final_answer, "ì¼ë°˜ LLM"
    
    # ê¸°ì¡´ ë©”ì†Œë“œë“¤ (try_rag_answer, try_manual_rag, etc.) ìœ ì§€
    def test_basic_connection(self):
        """ê¸°ë³¸ OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            print("ğŸ” ê¸°ë³¸ OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸...")
            response = self.client.chat.completions.create(
                model=self.deployment,
                messages=[
                    {"role": "system", "content": "ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}
                ],
                max_tokens=50,
                temperature=0.3
            )
            print("âœ… ê¸°ë³¸ OpenAI ì—°ê²° ì„±ê³µ!")
            return True
        except Exception as e:
            print(f"âŒ ê¸°ë³¸ OpenAI ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    def try_rag_answer(self, query: str) -> Tuple[Optional[str], bool]:
        """Azure Search í†µí•© RAGë¡œ ë‹µë³€ ì‹œë„"""
        # ê¸°ì¡´ êµ¬í˜„ ìœ ì§€...
        return None, False
    
    def try_manual_rag(self, query: str) -> Tuple[Optional[str], bool]:
        """ìˆ˜ë™ RAG ë°ì´í„°ë¡œ ì‹œë„"""
        # ê¸°ì¡´ êµ¬í˜„ ìœ ì§€...
        return None, False
    
    def try_hardcoded_rag(self, query: str) -> Tuple[Optional[str], bool]:
        """í•˜ë“œì½”ë”©ëœ RAG ë°ì´í„°ë¡œ ì‹œë„"""
        # ê¸°ì¡´ êµ¬í˜„ ìœ ì§€...
        return None, False
    
    def is_inadequate_answer(self, answer: str) -> bool:
        """RAG ë‹µë³€ì´ ë¶€ì ì ˆí•œì§€ íŒë‹¨"""
        if not answer:
            return True
        
        answer_lower = answer.lower()
        for pattern in self.inadequate_patterns:
            if pattern in answer_lower:
                return True
        
        if len(answer.strip()) < 50:
            return True
        
        return False
    
    def post_process_answer(self, answer: str) -> str:
        """RAG ë‹µë³€ì˜ ì°¸ì¡°ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ë³€ê²½"""
        if not answer:
            return answer
        
        citation_patterns = [
            r'\[doc\d+\]', r'\[document\d+\]', r'\[source\d+\]',
            r'\[ë¬¸ì„œ\d+\]', r'\[ìë£Œ\d+\]', r'\[ì°¸ê³ \d+\]'
        ]
        
        processed_answer = answer
        for pattern in citation_patterns:
            processed_answer = re.sub(pattern, '', processed_answer)
        
        processed_answer = re.sub(r'\s+', ' ', processed_answer)
        processed_answer = re.sub(r'\n\s*\n', '\n\n', processed_answer)
        
        return processed_answer.strip()
    
    def add_credibility_footer(self, answer: str, source: str) -> str:
        """ë‹µë³€ì— ì‹ ë¢°ì„± ì •ë³´ ì¶”ê°€"""
        footer_options = {
            "RAG": "ğŸ“š *ì„œìš¸ì‹œ ê³µì‹ ì‹±í¬í™€ ëŒ€ì‘ ë§¤ë‰´ì–¼ ê¸°ë°˜*",
            "ìˆ˜ë™ RAG": "ğŸ” ì„œìš¸ì‹œ ì•ˆì „ê´€ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼", 
            "í•˜ë“œì½”ë”©ëœ RAG": "ğŸ“‹ ì„œìš¸ì‹œ ê° êµ¬ì²­ ê³µì‹ ì—°ë½ì²˜ ë° ëŒ€ì‘ ì ˆì°¨",
            "ì¼ë°˜ LLM": "ğŸ§  *ì¼ë°˜ì ì¸ ì•ˆì „ ìƒì‹ ë° ê°€ì´ë“œë¼ì¸*",
            "ì‹±í¬í™€ AI ë¶„ì„": "ğŸ¤– *Azure Custom Vision AI ë¶„ì„ ê²°ê³¼*"
        }
        
        footer = footer_options.get(source, "")
        if footer:
            return f"{answer}\n\n{footer}"
        return answer
    
    def answer_with_general_llm(self, query: str) -> str:
        """ì¼ë°˜ LLMìœ¼ë¡œ ë‹µë³€"""
        try:
            print("ğŸ” ì¼ë°˜ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±...")
            
            response = self.client.chat.completions.create(
                model=self.deployment,
                messages=[
                    {
                        "role": "system",
                        "content": """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì‹±í¬í™€ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
íŠ¹íˆ ì•ˆì „ ìˆ˜ì¹™, ì‹ ê³  ë°©ë²•, ì˜ˆë°© ì¡°ì¹˜ ë“±ì— ëŒ€í•´ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”."""
                    },
                    {
                        "role": "user",
                        "content": query
                    }
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            print("âœ… ì¼ë°˜ LLM ë‹µë³€ ìƒì„± ì„±ê³µ!")
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âŒ ì¼ë°˜ LLM ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return """ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. 

ë‹¤ìŒê³¼ ê°™ì´ ì‹œë„í•´ë³´ì„¸ìš”:
â€¢ ì ì‹œ í›„ ë‹¤ì‹œ ì§ˆë¬¸í•´ë³´ì„¸ìš”
â€¢ ì§ˆë¬¸ì„ ë” ê°„ë‹¨í•˜ê²Œ ë°”ê¿”ë³´ì„¸ìš”
â€¢ ê¸´ê¸‰í•œ ê²½ìš° 119 ë˜ëŠ” 120ìœ¼ë¡œ ì—°ë½í•˜ì„¸ìš”

ğŸ” **ìš°ë¦¬ ì„œë¹„ìŠ¤ ê¸°ëŠ¥**
â€¢ ì‹¤ì‹œê°„ ìœ„í—˜ë„ ì˜ˆì¸¡
â€¢ ìœ„í—˜ì§€ì—­ ì§€ë„ í™•ì¸
â€¢ ì•ˆì „ ê²½ë¡œ ì•ˆë‚´

ì„œë¹„ìŠ¤ ì´ìš©ì— ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤."""

# ì „ì—­ RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
rag_system = EnhancedRAGSystem()